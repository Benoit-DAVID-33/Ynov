{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf69929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chargement des fichiers CSV ---\n",
      "Fichier Employés chargé : (100000, 23)\n",
      "Fichier Time Series chargé : (1200000, 8)\n",
      "--- Correction et Adaptation des Colonnes ---\n",
      "Colonne 'Annee_Embauche' créée à partir de 'hire_date'.\n",
      "------------------------------\n",
      "Colonnes disponibles pour le TP :\n",
      "['EmployeeID', 'first_name', 'last_name', 'email', 'Departement', 'Sous_Departement', 'team', 'Grade', 'Region', 'hire_date', 'tenure_years', 'Salaire', 'bonus', 'total_compensation', 'Performance', 'skills_count', 'certifications_count', 'Satisfaction', 'remote_work_days', 'turnover_risk', 'active_projects', 'project_success_rate', 'last_promotion_date', 'Annee_Embauche', 'Competence_Score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"--- Chargement des fichiers CSV ---\")\n",
    "\n",
    "# 1. Chargement des données Employés\n",
    "try:\n",
    "    df = pd.read_csv('advanced_employees.csv')\n",
    "    print(f\"Fichier Employés chargé : {df.shape}\")\n",
    "    \n",
    "    # Optimisation immédiate des types pour la mémoire (comme demandé dans le TP)\n",
    "    # On convertit les colonnes textuelles en 'category' si elles ont peu de valeurs uniques\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        if len(df[col].unique()) < len(df) * 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERREUR : Le fichier 'advanced_employees.csv' est introuvable.\")\n",
    "\n",
    "# 2. Chargement des Time Series (Données temporelles)\n",
    "try:\n",
    "    ts_df = pd.read_csv('employee_timeseries.csv')\n",
    "    \n",
    "    # Conversion de la date (crucial pour les exercices Q19-Q21)\n",
    "    # On suppose qu'il y a une colonne 'Date' ou 'date'. On essaie de la trouver.\n",
    "    col_date = 'Date' if 'Date' in ts_df.columns else 'date'\n",
    "    \n",
    "    if col_date in ts_df.columns:\n",
    "        ts_df[col_date] = pd.to_datetime(ts_df[col_date])\n",
    "        # On renomme en 'Date' pour standardiser la suite du code\n",
    "        ts_df = ts_df.rename(columns={col_date: 'Date'})\n",
    "    \n",
    "    print(f\"Fichier Time Series chargé : {ts_df.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERREUR : Le fichier 'employee_timeseries.csv' est introuvable.\")\n",
    "\n",
    "print(\"--- Correction et Adaptation des Colonnes ---\")\n",
    "\n",
    "# 1. Dictionnaire de mapping (Vos colonnes -> Colonnes du TP)\n",
    "mapping = {\n",
    "    'employee_id': 'EmployeeID',\n",
    "    'region': 'Region',\n",
    "    'main_department': 'Departement',\n",
    "    'sub_department': 'Sous_Departement',\n",
    "    'grade': 'Grade',\n",
    "    'base_salary': 'Salaire',             # On utilise le salaire de base\n",
    "    'performance_score': 'Performance',\n",
    "    'satisfaction_score': 'Satisfaction',\n",
    "    # Note : 'hire_date' sera traité séparément pour extraire l'année\n",
    "}\n",
    "\n",
    "df = df.rename(columns=mapping)\n",
    "\n",
    "# 2. Transformation de la date d'embauche en Année (Entier)\n",
    "# Le TP filtre sur l'année (ex: > 2020), il faut donc extraire l'année de la date complète\n",
    "if 'hire_date' in df.columns:\n",
    "    df['Annee_Embauche'] = pd.to_datetime(df['hire_date']).dt.year\n",
    "    print(\"Colonne 'Annee_Embauche' créée à partir de 'hire_date'.\")\n",
    "else:\n",
    "    # Fallback si hire_date n'existe pas (peu probable vu votre liste)\n",
    "    df['Annee_Embauche'] = 2024 - df['tenure_years'].astype(int)\n",
    "\n",
    "# 3. Création de la colonne 'Competence_Score' (Manquante dans votre CSV mais requise en Q8)\n",
    "# On peut utiliser 'project_success_rate' ou 'skills_count' comme proxy\n",
    "# Ici, on normalise 'project_success_rate' (supposé en % ou 0-1) vers 0-1\n",
    "if 'project_success_rate' in df.columns:\n",
    "    df['Competence_Score'] = df['project_success_rate'] / 100.0 if df['project_success_rate'].max() > 1 else df['project_success_rate']\n",
    "else:\n",
    "    df['Competence_Score'] = np.random.rand(len(df))\n",
    "\n",
    "# 4. Vérification des types pour l'optimisation mémoire\n",
    "cols_cat = ['Region', 'Departement', 'Sous_Departement', 'Grade', 'team']\n",
    "for col in cols_cat:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Colonnes disponibles pour le TP :\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4d1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EmployeeID', 'first_name', 'last_name', 'email', 'Departement', 'Sous_Departement', 'team', 'Grade', 'Region', 'hire_date', 'tenure_years', 'Salaire', 'bonus', 'total_compensation', 'Performance', 'skills_count', 'certifications_count', 'Satisfaction', 'remote_work_days', 'turnover_risk', 'active_projects', 'project_success_rate', 'last_promotion_date', 'Annee_Embauche', 'Competence_Score']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.1 MultiIndex & Indexation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benoi\\AppData\\Local\\Temp\\ipykernel_8084\\2809626240.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  stats_level = df_multi['Salaire'].groupby(level=[0, 1, 2]).describe()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Managers Europe > 2020 : 1375\n",
      "Q2: Sélection optimisée en 0.0138s - 9455 résultats\n",
      "Q3: Aperçu agrégation dynamique :\n",
      "                    EmployeeID       Salaire  Corr_Perf_Salaire\n",
      "Region Departement                                             \n",
      "Africa Finance            6620  66958.975741          -0.002075\n",
      "       Marketing          6692  66336.719268          -0.009416\n",
      "       Technologie        6728  66774.199341          -0.002585\n",
      "Asia   Finance            6786  66470.099329           0.000756\n",
      "       Marketing          6566  67032.057769           0.004544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benoi\\AppData\\Local\\Temp\\ipykernel_8084\\2809626240.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg_q3 = df_multi.groupby(level=[0, 1]).agg(metrics)\n",
      "C:\\Users\\benoi\\AppData\\Local\\Temp\\ipykernel_8084\\2809626240.py:42: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg_q3['Corr_Perf_Salaire'] = df_multi.groupby(level=[0, 1]).apply(\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 10.1 MultiIndex & Indexation ---\")\n",
    "\n",
    "# Q1 : Structure Hiérarchique Complexe & xs\n",
    "df_multi = df.set_index(['Region', 'Departement', 'Grade', 'Annee_Embauche']).sort_index()\n",
    "stats_level = df_multi['Salaire'].groupby(level=[0, 1, 2]).describe()\n",
    "\n",
    "# Extraction complexe avec xs (Cross Section)\n",
    "# Niveau 0 (Europe) et Niveau 2 (Manager) fixes. Niveau 1 et 3 libres.\n",
    "# Attention: xs renvoie une vue, on filtre ensuite sur l'année (index niveau 3)\n",
    "managers_europe = df_multi.xs(('Europe', 'Manager'), level=['Region', 'Grade'], drop_level=False)\n",
    "managers_europe_post2020 = managers_europe[managers_europe.index.get_level_values('Annee_Embauche') > 2020]\n",
    "\n",
    "print(f\"Q1: Managers Europe > 2020 : {len(managers_europe_post2020)}\")\n",
    "\n",
    "# Q2 : IndexSlice\n",
    "idx = pd.IndexSlice\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "target_depts = ['Technologie', 'Finance']\n",
    "target_grades = ['Senior', 'Lead']\n",
    "\n",
    "# .loc avec IndexSlice sur les tuples de l'index + masque booléen sur colonne\n",
    "selection = df_multi.loc[idx[:, target_depts, target_grades, :], :]\n",
    "resultat_q2 = selection[selection['Performance'] > 7.5]\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Q2: Sélection optimisée en {end - start:.4f}s - {len(resultat_q2)} résultats\")\n",
    "\n",
    "# Q3 : Agrégation Dynamique\n",
    "metrics = {\n",
    "    'EmployeeID': 'count', # Effectif\n",
    "    'Salaire': 'std',      # Ecart-type\n",
    "    # Pour la corrélation et turnover, on le fait souvent séparément ou via apply custom (plus lent)\n",
    "}\n",
    "agg_q3 = df_multi.groupby(level=[0, 1]).agg(metrics)\n",
    "# Calcul corrélation vectorisé (plus rapide que groupby apply)\n",
    "agg_q3['Corr_Perf_Salaire'] = df_multi.groupby(level=[0, 1]).apply(\n",
    "    lambda x: x['Performance'].corr(x['Salaire'])\n",
    ")\n",
    "\n",
    "print(\"Q3: Aperçu agrégation dynamique :\")\n",
    "print(agg_q3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d607b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.2 Broadcasting Avancé ---\n",
      "Q4: Matrice calculée shape (1000, 1000)\n",
      "Q5: Score moyen calculé: 0.00\n",
      "Q6: Nombre de 'Gems' (Overperformers underpaid): 29869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benoi\\AppData\\Local\\Temp\\ipykernel_8084\\2430156411.py:31: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  dept_means = df.groupby('Departement')[['Salaire', 'Performance']].transform('mean')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 10.2 Broadcasting Avancé ---\")\n",
    "\n",
    "# Q4 : Matrice de Similarités (Optimisation Mémoire)\n",
    "# NOTE : 100k x 100k float64 = 80 Go de RAM. Impossible sur un laptop standard.\n",
    "# Solution : On démontre sur un chunk de 1000 employés (Chunking).\n",
    "subset = df[['Salaire', 'Performance', 'Satisfaction']].iloc[:1000].values\n",
    "# Normalisation préalable nécessaire pour distance euclidienne\n",
    "subset_norm = (subset - subset.mean(axis=0)) / subset.std(axis=0)\n",
    "\n",
    "# Broadcasting: (N, 1, D) - (1, N, D) -> (N, N, D)\n",
    "# Puis somme des carrés sur l'axe D et racine carrée\n",
    "dist_matrix = np.sqrt(((subset_norm[:, np.newaxis, :] - subset_norm[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "print(f\"Q4: Matrice calculée shape {dist_matrix.shape}\")\n",
    "\n",
    "# Q5 : Normalisation Multi-Dimensionnelle pondérée\n",
    "# Array (100k, 3)\n",
    "data_metrics = df[['Salaire', 'Performance', 'Satisfaction']].values\n",
    "means = data_metrics.mean(axis=0)\n",
    "stds = data_metrics.std(axis=0)\n",
    "weights = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "# Broadcasting: (N, 3) - (3,) / (3,)\n",
    "z_scores = (data_metrics - means) / stds\n",
    "# Dot product pour appliquer les poids: (N, 3) @ (3,) -> (N,)\n",
    "final_scores = z_scores @ weights\n",
    "df['Weighted_Score'] = final_scores\n",
    "print(f\"Q5: Score moyen calculé: {final_scores.mean():.2f}\")\n",
    "\n",
    "# Q6 : Benchmarks Départementaux (Optimisé)\n",
    "# On calcule les moyennes par département\n",
    "dept_means = df.groupby('Departement')[['Salaire', 'Performance']].transform('mean')\n",
    "# Calcul vectorisé direct\n",
    "ecarts = df[['Salaire', 'Performance']] - dept_means\n",
    "over_performers = df[(ecarts['Performance'] > 0) & (ecarts['Salaire'] < 0)] # Performent mieux que la moyenne mais payés moins\n",
    "\n",
    "print(f\"Q6: Nombre de 'Gems' (Overperformers underpaid): {len(over_performers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00563713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.3 Einstein Summation (np.einsum) ---\n",
      "Dimensions du Tensor : 3 Départements x 7 Grades x 5 Skills\n",
      "Q7: Matrice similarité Départements (Extrait 5x5):\n",
      "[[ 9.75022019  7.37525116  7.94548028]\n",
      " [ 7.37525116 10.80263398  8.63819867]\n",
      " [ 7.94548028  8.63819867 11.02568111]]\n",
      "Q8: Shape après projection einsum: (100000, 2)\n",
      "Q9: Différence max Einsum vs Numpy: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 10.3 Einstein Summation (np.einsum) ---\")\n",
    "\n",
    "# --- CORRECTION : On récupère les dimensions depuis vos données réelles ---\n",
    "# Au lieu de listes hardcodées, on compte les valeurs uniques dans le DataFrame\n",
    "n_depts = df['Departement'].nunique()\n",
    "n_grades = df['Grade'].nunique()\n",
    "n_skills = 5 # Arbitraire pour l'exercice\n",
    "\n",
    "print(f\"Dimensions du Tensor : {n_depts} Départements x {n_grades} Grades x {n_skills} Skills\")\n",
    "\n",
    "# Tenseur simulé : Dept x Grade x Compétence Moyenne\n",
    "# (On simule cette matrice car elle n'existe pas telle quelle dans le CSV)\n",
    "tensor_dgc = np.random.rand(n_depts, n_grades, n_skills)\n",
    "\n",
    "# Q7 : Contraction de Tensors\n",
    "# Similarité entre départements : on \"écrase\" grade et skills.\n",
    "# Produit scalaire entre Dept i et Dept j sur les axes Grade(k) et Skill(l)\n",
    "# Résultat attendu : Matrice carrée (n_depts x n_depts)\n",
    "dept_sim = np.einsum('ikl,jkl->ij', tensor_dgc, tensor_dgc)\n",
    "\n",
    "print(\"Q7: Matrice similarité Départements (Extrait 5x5):\")\n",
    "print(dept_sim[:5, :5]) \n",
    "\n",
    "# Q8 : Projection 2D\n",
    "# On s'assure que les colonnes existent (Competence_Score a été créé dans la correction précédente)\n",
    "features = df[['Performance', 'Satisfaction', 'Competence_Score']].values # (N, 3)\n",
    "proj_matrix = np.random.randn(3, 2) # (3, 2)\n",
    "\n",
    "# Projection: (N, 3) x (3, 2) -> (N, 2)\n",
    "# 'ij,jk->ik' est l'équivalent du dot product classique\n",
    "projections = np.einsum('ij,jk->ik', features, proj_matrix)\n",
    "print(f\"Q8: Shape après projection einsum: {projections.shape}\")\n",
    "\n",
    "# Q9 : Covariance optimisée avec einsum\n",
    "# Centrage des données (Moyenne = 0)\n",
    "X = features - features.mean(axis=0) \n",
    "N_samples = X.shape[0]\n",
    "\n",
    "# Covariance: (X.T @ X) / (N-1)\n",
    "# 'ki,kj->ij' signifie : pour chaque ligne k, on prend feat i et feat j, on multiplie et on somme sur k.\n",
    "cov_einsum = np.einsum('ki,kj->ij', X, X) / (N_samples - 1)\n",
    "\n",
    "# Comparaison avec NumPy classique\n",
    "cov_numpy = np.cov(features, rowvar=False)\n",
    "\n",
    "print(f\"Q9: Différence max Einsum vs Numpy: {np.abs(cov_einsum - cov_numpy).max():.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4288dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.4 UFuncs & Numba ---\n",
      "Calculs effectués sur 100000 employés.\n",
      "Q10: Calcul ROI (Numba): 0.0032s\n",
      "Q11: Market Value moyenne: 147,094.60\n",
      "Q12: Monte Carlo terminé (10000 scénarios sur 1000 employés).\n",
      "     P95 moyen: 132,284.56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import vectorize, float32, float64\n",
    "\n",
    "print(\"\\n--- 10.4 UFuncs & Numba ---\")\n",
    "\n",
    "# --- CORRECTION : On récupère N depuis la taille réelle du DataFrame ---\n",
    "N = len(df)\n",
    "print(f\"Calculs effectués sur {N} employés.\")\n",
    "\n",
    "# Q10 : UFunc ROI Formation (Numba)\n",
    "# target='parallel' permet d'utiliser tous les cœurs CPU\n",
    "@vectorize([float32(float32, float32, float32, float32)], target='parallel')\n",
    "def ufunc_roi(delta_perf, salaire, cout, duree):\n",
    "    if cout * duree == 0:\n",
    "        return 0.0\n",
    "    return (delta_perf * salaire) / (cout * duree)\n",
    "\n",
    "# Données factices pour test (On utilise N ici)\n",
    "d_perf = np.random.uniform(0.1, 0.5, N).astype(np.float32)\n",
    "couts = np.random.uniform(1000, 5000, N).astype(np.float32)\n",
    "durees = np.random.uniform(1, 10, N).astype(np.float32)\n",
    "salaires = df['Salaire'].values.astype(np.float32)\n",
    "\n",
    "start = time.time()\n",
    "# Premier appel (inclut la compilation JIT)\n",
    "roi_result = ufunc_roi(d_perf, salaires, couts, durees)\n",
    "print(f\"Q10: Calcul ROI (Numba): {time.time() - start:.4f}s\")\n",
    "\n",
    "# Q11 : Market Value (NumPy pur vectorisé)\n",
    "# Formule complexe arbitraire\n",
    "# Note : On utilise .values pour passer en NumPy pur (plus rapide que Pandas Series)\n",
    "anciennete = 2025 - df['Annee_Embauche'].values\n",
    "\n",
    "# Calcul vectorisé\n",
    "mv_vec = (df['Salaire'].values * 0.8) + \\\n",
    "         (df['Performance'].values * 5000) + \\\n",
    "         (anciennete * 2000)\n",
    "\n",
    "print(f\"Q11: Market Value moyenne: {mv_vec.mean():,.2f}\")\n",
    "\n",
    "# Q12 : Simulation Monte Carlo Vectorisée\n",
    "n_scenarios = 10000\n",
    "\n",
    "# Pour éviter de saturer la mémoire si N est très grand, on fait le test sur 1000 employés\n",
    "n_subset = min(N, 1000) \n",
    "current_salaries = df['Salaire'].values[:n_subset].reshape(-1, 1)\n",
    "\n",
    "# Broadcasting: (n_subset, 1) * (1, Scenarios) -> (n_subset, Scenarios)\n",
    "growth_rates = np.random.normal(1.03, 0.02, n_scenarios).astype(np.float32)\n",
    "\n",
    "simulated_salaries = current_salaries * growth_rates\n",
    "\n",
    "# Percentiles sur l'axe des scénarios (axis=1)\n",
    "p95 = np.percentile(simulated_salaries, 95, axis=1)\n",
    "\n",
    "print(f\"Q12: Monte Carlo terminé ({n_scenarios} scénarios sur {n_subset} employés).\")\n",
    "print(f\"     P95 moyen: {p95.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51bb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.5 Array Structurés & Optimisation Mémoire ---\n",
      "Q13: Mémoire Pandas: 7.82 MB vs Structured: 1.14 MB\n",
      "Q14: Moyenne salaire (Perf > 8): 125867.18 (Temps: 0.00455s)\n",
      "Q15: Moyenne partielle via Memmap: 123053.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 10.5 Array Structurés & Optimisation Mémoire ---\")\n",
    "\n",
    "N = len(df) # On s'assure que N est défini\n",
    "\n",
    "# Q13 : Conversion en Structured Array\n",
    "# On définit des types précis : 'i4' (int32), 'f4' (float32)\n",
    "dtype_spec = [('id', 'i4'), ('salary', 'f4'), ('perf', 'f4')]\n",
    "struct_arr = np.zeros(N, dtype=dtype_spec)\n",
    "\n",
    "# --- CORRECTION ICI ---\n",
    "# On nettoie l'ID : On enlève 'EMP' et on convertit en entier\n",
    "try:\n",
    "    # Si les IDs sont du type \"EMP12345\"\n",
    "    struct_arr['id'] = df['EmployeeID'].astype(str).str.replace('EMP', '', regex=False).astype(int).values\n",
    "except ValueError:\n",
    "    # Si les IDs sont complexes (ex: \"A-12-B\"), on utilise simplement l'index (0, 1, 2...)\n",
    "    print(\"IDs non convertibles en entiers. Utilisation de l'index à la place.\")\n",
    "    struct_arr['id'] = df.index.values\n",
    "\n",
    "# Remplissage des autres colonnes\n",
    "struct_arr['salary'] = df['Salaire'].values\n",
    "struct_arr['perf'] = df['Performance'].values\n",
    "\n",
    "# Comparaison Mémoire\n",
    "mem_pandas = df[['EmployeeID', 'Salaire', 'Performance']].memory_usage(deep=True).sum()\n",
    "mem_struct = struct_arr.nbytes\n",
    "print(f\"Q13: Mémoire Pandas: {mem_pandas/1024**2:.2f} MB vs Structured: {mem_struct/1024**2:.2f} MB\")\n",
    "\n",
    "# Q14 : Calculs natifs sur Structured Array\n",
    "start = time.time()\n",
    "# Syntaxe numpy : tableau[condition]['colonne'].mean()\n",
    "avg_salary_high_perf = struct_arr[struct_arr['perf'] > 8]['salary'].mean()\n",
    "print(f\"Q14: Moyenne salaire (Perf > 8): {avg_salary_high_perf:.2f} (Temps: {time.time()-start:.5f}s)\")\n",
    "\n",
    "# Q15 : Memory Mapping (np.memmap)\n",
    "filename = 'employees.dat'\n",
    "\n",
    "# On écrit le tableau sur disque\n",
    "# Attention : memmap a besoin d'une forme (shape) fixe.\n",
    "# On prend les 3 colonnes numériques : Salaire, Performance, Satisfaction\n",
    "cols_mmap = ['Salaire', 'Performance', 'Satisfaction']\n",
    "data_to_mmap = df[cols_mmap].fillna(0).values.astype('float32') # fillna pour éviter erreur sur NaN\n",
    "\n",
    "fp = np.memmap(filename, dtype='float32', mode='w+', shape=(N, 3))\n",
    "fp[:] = data_to_mmap\n",
    "fp.flush() # Écriture physique sur le disque\n",
    "\n",
    "# Lecture partielle sans charger 100% en RAM\n",
    "# Imaginez que le fichier fait 100 Go, cette ligne ne charge RIEN en RAM\n",
    "new_fp = np.memmap(filename, dtype='float32', mode='r', shape=(N, 3))\n",
    "\n",
    "# Le chargement en RAM se fait uniquement au moment de l'accès (ici les 1000 derniers)\n",
    "partial_mean = new_fp[-1000:, 0].mean() # Colonne 0 = Salaire\n",
    "print(f\"Q15: Moyenne partielle via Memmap: {partial_mean:.2f}\")\n",
    "\n",
    "# Nettoyage fichier (pour ne pas laisser de traces)\n",
    "try:\n",
    "    del new_fp, fp # Il faut fermer les pointeurs avant de supprimer\n",
    "    os.remove(filename)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42981825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.6 & 10.7 Agrégations & Time Series (Corrigé V2) ---\n",
      "Correction : Renommage de 'employee_id' en 'EmployeeID'.\n",
      "--> Colonne utilisée pour les statistiques : 'performance_score'\n",
      "Q19: Stats mensuelles (extrait):\n",
      "                 mean       std\n",
      "Date                          \n",
      "1970-01-31  7.453307  1.454412\n",
      "Q20: EWMA calculé avec succès.\n",
      "\n",
      "Q16: Employés à risque par Dept (Extrait):\n",
      " Departement\n",
      "Finance        16675\n",
      "Marketing      16615\n",
      "Technologie    16707\n",
      "Name: Low_Salary, dtype: int64\n",
      "Q17: Rang calculé. Max rang: 4830.5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 10.6 & 10.7 Agrégations & Time Series (Corrigé V2) ---\")\n",
    "\n",
    "# --- PARTIE 1 : Correction des noms de colonnes dans ts_df ---\n",
    "# On s'assure que l'ID est bien 'EmployeeID'\n",
    "if 'employee_id' in ts_df.columns:\n",
    "    print(\"Correction : Renommage de 'employee_id' en 'EmployeeID'.\")\n",
    "    ts_df = ts_df.rename(columns={'employee_id': 'EmployeeID'})\n",
    "\n",
    "# --- PARTIE 2 : Gestion de la Date ---\n",
    "possible_date_cols = ['date', 'Date', 'timestamp', 'month', 'period']\n",
    "found_col = None\n",
    "for col in possible_date_cols:\n",
    "    if col in ts_df.columns:\n",
    "        found_col = col\n",
    "        break\n",
    "\n",
    "if found_col:\n",
    "    if found_col != 'Date':\n",
    "        ts_df = ts_df.rename(columns={found_col: 'Date'})\n",
    "    \n",
    "    # Conversion date\n",
    "    ts_df['Date'] = pd.to_datetime(ts_df['Date'])\n",
    "    \n",
    "    # --- PARTIE 3 : Sélection Intelligente de la Métrique ---\n",
    "    # On cherche une colonne numérique qui N'EST PAS une date ou un ID\n",
    "    cols_num = ts_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    ignore_list = ['EmployeeID', 'Date', 'year', 'Year', 'month', 'Month', 'day', 'Day']\n",
    "    \n",
    "    # On garde seulement les colonnes qui ne sont pas dans la liste d'exclusion\n",
    "    valid_metrics = [c for c in cols_num if c not in ignore_list]\n",
    "    \n",
    "    if valid_metrics:\n",
    "        target_col = valid_metrics[0] # On prend la première vraie métrique trouvée\n",
    "        print(f\"--> Colonne utilisée pour les statistiques : '{target_col}'\")\n",
    "        \n",
    "        # On met l'index seulement maintenant pour le resample\n",
    "        ts_indexed = ts_df.set_index('Date')\n",
    "\n",
    "        # Q19 : Resample\n",
    "        monthly_stats = ts_indexed.resample('ME')[target_col].agg(['mean', 'std'])\n",
    "        print(\"Q19: Stats mensuelles (extrait):\\n\", monthly_stats.head(3))\n",
    "\n",
    "        # Q20 : Rolling Windows (EWM)\n",
    "        # On utilise ts_df (avec index reset) pour garantir l'alignement\n",
    "        # groupby sur 'EmployeeID' nécessite que la colonne existe\n",
    "        ts_indexed['EWMA_30'] = ts_indexed.groupby('EmployeeID', observed=True)[target_col].transform(\n",
    "            lambda x: x.ewm(span=30).mean()\n",
    "        )\n",
    "        print(\"Q20: EWMA calculé avec succès.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ATTENTION : Aucune colonne de 'score' ou 'valeur' trouvée (seulement des dates/IDs).\")\n",
    "        print(\"Colonnes disponibles:\", ts_df.columns)\n",
    "\n",
    "else:\n",
    "    print(\"ERREUR : Colonne date introuvable.\")\n",
    "\n",
    "# --- PARTIE 4 : Q16 & Q17 (Sur le DataFrame Principal df) ---\n",
    "# Q16\n",
    "medianes_dept = df.groupby('Departement', observed=True)['Salaire'].transform('median')\n",
    "df['Low_Salary'] = df['Salaire'] < medianes_dept\n",
    "risk_count = df.groupby('Departement', observed=True)['Low_Salary'].sum()\n",
    "print(\"\\nQ16: Employés à risque par Dept (Extrait):\\n\", risk_count.head(3))\n",
    "\n",
    "# Q17\n",
    "df['Rank_Dept_Grade'] = df.groupby(['Departement', 'Grade'], observed=True)['Salaire'].rank(ascending=False)\n",
    "print(f\"Q17: Rang calculé. Max rang: {df['Rank_Dept_Grade'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cf80e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "     -------------------------------------- 308.4/308.4 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4ca10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "     ---------------------------------------- 8.9/8.9 MB 40.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.3.5)\n",
      "Collecting scipy>=1.8.0\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "     --------------------------------------- 38.7/38.7 MB 28.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de0a839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10.8 Performance & 10.9 Intégration ---\n",
      "Q23: Optimisation: 29.7MB -> 22.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q24: Corrélations parallèles: [np.float64(0.002650458233334195), np.float64(-0.012320876456632157), np.float64(0.001767270400794949), np.float64(0.000804449534134177)]\n",
      "Q26: Anomalies détectées sur 10k employés: 100\n",
      "Q27: Masse salariale actuelle: 12,576,939,981\n",
      "Q27: Masse salariale projetée (3 ans): 16,502,621,834\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"\\n--- 10.8 Performance & 10.9 Intégration ---\")\n",
    "\n",
    "# Q23 : Optimisation Automatique (Downcasting)\n",
    "def optimize_dataframe(dframe):\n",
    "    start_mem = dframe.memory_usage(deep=True).sum()\n",
    "    for col in dframe.select_dtypes(include=['float64']).columns:\n",
    "        dframe[col] = pd.to_numeric(dframe[col], downcast='float')\n",
    "    for col in dframe.select_dtypes(include=['int64']).columns:\n",
    "        dframe[col] = pd.to_numeric(dframe[col], downcast='integer')\n",
    "    for col in dframe.select_dtypes(include=['object']).columns:\n",
    "        num_unique = len(dframe[col].unique())\n",
    "        num_total = len(dframe[col])\n",
    "        if num_unique / num_total < 0.5: # Si peu de valeurs uniques -> Category\n",
    "            dframe[col] = dframe[col].astype('category')\n",
    "    end_mem = dframe.memory_usage(deep=True).sum()\n",
    "    print(f\"Q23: Optimisation: {start_mem/1024**2:.1f}MB -> {end_mem/1024**2:.1f}MB\")\n",
    "    return dframe\n",
    "\n",
    "df = optimize_dataframe(df.copy()) # On copie pour pas casser le df original si besoin\n",
    "\n",
    "# Q24 : Parallel Processing (Joblib)\n",
    "# Calcul de corrélation sur des sous-ensembles (exemple factice)\n",
    "def process_correlation(subset_df):\n",
    "    return subset_df[['Salaire', 'Performance']].corr().iloc[0, 1]\n",
    "\n",
    "# On découpe le DF en 4 chunks\n",
    "chunks = np.array_split(df, 4)\n",
    "results_parallel = Parallel(n_jobs=2)(delayed(process_correlation)(chunk) for chunk in chunks)\n",
    "print(f\"Q24: Corrélations parallèles: {results_parallel}\")\n",
    "\n",
    "# Q26 : Detection d'anomalies (Isolation Forest)\n",
    "# On détecte les profils bizarres (Salaire vs Perf vs Satisfaction)\n",
    "model = IsolationForest(n_estimators=100, contamination=0.01, n_jobs=-1)\n",
    "# On prend un sample pour aller vite\n",
    "X_iso = df[['Salaire', 'Performance', 'Satisfaction']].iloc[:10000]\n",
    "outliers = model.fit_predict(X_iso)\n",
    "print(f\"Q26: Anomalies détectées sur 10k employés: {np.sum(outliers == -1)}\")\n",
    "\n",
    "# Q27 : Forecasting (Simple Vectorisé)\n",
    "# Projection naïve: Salaire * (1 + inflation + (perf/100))\n",
    "growth_factor = 1.02 + (df['Performance'] / 100.0) # 2% base + perf%\n",
    "forecast_3y = df['Salaire'] * (growth_factor ** 3)\n",
    "print(f\"Q27: Masse salariale actuelle: {df['Salaire'].sum():,.0f}\")\n",
    "print(f\"Q27: Masse salariale projetée (3 ans): {forecast_3y.sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce3c466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "     --------------------------------------- 28.1/28.1 MB 34.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62e2c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp311-cp311-win_amd64.whl (671 kB)\n",
      "     -------------------------------------- 671.0/671.0 kB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.3.5)\n",
      "Collecting cramjam>=2.3\n",
      "  Downloading cramjam-2.11.0-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 37.0 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "     ------------------------------------- 201.0/201.0 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Installing collected packages: fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.11.0 fsspec-2025.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "721d981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.10 Q30 : Compression et Archivage (Moteur FastParquet) ---\n",
      "Format     | Taille (MB)  | Temps (s)  | Ratio vs CSV\n",
      "--------------------------------------------------\n",
      "CSV        | 16.05        | 1.1562     | 1.0x      \n",
      "Parquet    | 7.55         | 0.5863     | 2.1       x\n",
      "Feather    | 11.61        | 0.1606     | 1.4       x\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"--- 10.10 Q30 : Compression et Archivage (Moteur FastParquet) ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Préparation des données\n",
    "    # On recharge pour être propre\n",
    "    df_bonus = pd.read_csv('advanced_employees.csv')\n",
    "    \n",
    "    # Nettoyage des types pour éviter les soucis de compatibilité\n",
    "    # Fastparquet gère bien les catégories, mais on assure le coup\n",
    "    for col in df_bonus.select_dtypes(['object']).columns:\n",
    "        df_bonus[col] = df_bonus[col].astype(str)\n",
    "\n",
    "    # --- TEST 1 : CSV (Référence) ---\n",
    "    start = time.time()\n",
    "    df_bonus.to_csv('temp.csv', index=False)\n",
    "    t_csv = time.time() - start\n",
    "    s_csv = os.path.getsize('temp.csv')\n",
    "\n",
    "    # --- TEST 2 : PARQUET (Compression Snappy par défaut) ---\n",
    "    # NOTE : On utilise engine='fastparquet' ici !\n",
    "    start = time.time()\n",
    "    df_bonus.to_parquet('temp.parquet', engine='fastparquet', compression='snappy')\n",
    "    t_parq = time.time() - start\n",
    "    s_parq = os.path.getsize('temp.parquet')\n",
    "\n",
    "    # --- TEST 3 : FEATHER (Format ultra-rapide non compressé) ---\n",
    "    # Nécessite pyarrow, mais Feather est souvent plus stable que Parquet\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_bonus.to_feather('temp.feather')\n",
    "        t_feath = time.time() - start\n",
    "        s_feath = os.path.getsize('temp.feather')\n",
    "    except:\n",
    "        t_feath, s_feath = 0, 0 # Si ça plante, on ignore\n",
    "\n",
    "    # --- RÉSULTATS ---\n",
    "    print(f\"{'Format':<10} | {'Taille (MB)':<12} | {'Temps (s)':<10} | {'Ratio vs CSV':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'CSV':<10} | {s_csv/1024**2:<12.2f} | {t_csv:<10.4f} | {'1.0x':<10}\")\n",
    "    print(f\"{'Parquet':<10} | {s_parq/1024**2:<12.2f} | {t_parq:<10.4f} | {s_csv/s_parq:<10.1f}x\")\n",
    "    if s_feath > 0:\n",
    "        print(f\"{'Feather':<10} | {s_feath/1024**2:<12.2f} | {t_feath:<10.4f} | {s_csv/s_feath:<10.1f}x\")\n",
    "\n",
    "    # Nettoyage\n",
    "    for f in ['temp.csv', 'temp.parquet', 'temp.feather']:\n",
    "        if os.path.exists(f): os.remove(f)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"ERREUR : Veuillez installer fastparquet (%pip install fastparquet)\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04c5047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
      "     ---------------------------------------- 9.9/9.9 MB 24.3 MB/s eta 0:00:00\n",
      "Collecting narwhals>=1.15.1\n",
      "  Downloading narwhals-2.12.0-py3-none-any.whl (425 kB)\n",
      "     ------------------------------------- 425.0/425.0 kB 13.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from plotly) (25.0)\n",
      "Installing collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.12.0 plotly-6.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script plotly_get_chrome.exe is installed in 'c:\\Users\\benoi\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d45bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbformat\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Collecting jsonschema>=2.6\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.0/90.0 kB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from nbformat) (5.9.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from nbformat) (5.14.3)\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.6/67.6 kB ? eta 0:00:00\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.29.0-cp311-cp311-win_amd64.whl (235 kB)\n",
      "     ------------------------------------- 235.7/235.7 kB 15.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "Installing collected packages: fastjsonschema, rpds-py, attrs, referencing, jsonschema-specifications, jsonschema, nbformat\n",
      "Successfully installed attrs-25.4.0 fastjsonschema-2.21.2 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 nbformat-5.10.4 referencing-0.37.0 rpds-py-0.29.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script jsonschema.exe is installed in 'c:\\Users\\benoi\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-trust.exe is installed in 'c:\\Users\\benoi\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af9216b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.10.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (7.1.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from nbformat) (5.9.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (9.7.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (4.15.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\benoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.29.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.5.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\benoi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade nbformat ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "144ff9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.10 Q28 : Visualisation (Méthode Sauvegarde) ---\n",
      "✅ Succès ! Le graphique a été sauvegardé dans : graphique_sunburst.html\n",
      "👉 Regardez dans le dossier de votre projet (à gauche dans VS Code) et ouvrez ce fichier.\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- 10.10 Q28 : Visualisation (Méthode Sauvegarde) ---\")\n",
    "\n",
    "if 'df' in locals():\n",
    "    # 1. Préparation\n",
    "    df_viz = df.groupby(['Region', 'Departement', 'Grade'], observed=True).size().reset_index(name='Count')\n",
    "    df_viz = df_viz[df_viz['Count'] > 0]\n",
    "\n",
    "    # 2. Création\n",
    "    fig = px.sunburst(\n",
    "        df_viz, \n",
    "        path=['Region', 'Departement', 'Grade'], \n",
    "        values='Count',\n",
    "        title=\"Organisation Hiérarchique des Effectifs\",\n",
    "        color='Count',\n",
    "        color_continuous_scale='RdBu',\n",
    "        width=800, height=800\n",
    "    )\n",
    "\n",
    "    # 3. SOLUTION DE CONTOURNEMENT : Sauvegarde en fichier\n",
    "    # Cela évite l'erreur \"nbformat\" car on n'affiche pas dans le notebook directement\n",
    "    output_file = \"graphique_sunburst.html\"\n",
    "    fig.write_html(output_file)\n",
    "    \n",
    "    print(f\"✅ Succès ! Le graphique a été sauvegardé dans : {output_file}\")\n",
    "    print(\"👉 Regardez dans le dossier de votre projet (à gauche dans VS Code) et ouvrez ce fichier.\")\n",
    "    \n",
    "    # Tentative d'ouverture automatique dans votre navigateur web\n",
    "    import webbrowser\n",
    "    try:\n",
    "        webbrowser.open('file://' + os.path.realpath(output_file))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"Erreur : Variable 'df' manquante.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77b05a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.10 Q29 : API SQL sur Structured Array ---\n",
      "Test API Custom...\n",
      "Test Pandas Query...\n",
      "\n",
      "Résultats Benchmark :\n",
      "Custom Numpy SQL : 0.22035 s\n",
      "Pandas Query     : 0.04743 s\n",
      "Vainqueur        : Pandas\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 10.10 Q29 : API SQL sur Structured Array ---\")\n",
    "\n",
    "class TableSQL:\n",
    "    def __init__(self, df_input):\n",
    "        # Conversion en structured array optimisé\n",
    "        records = df_input.to_records(index=False)\n",
    "        self.data = np.array(records, dtype=records.dtype)\n",
    "    \n",
    "    def select(self, cols=None):\n",
    "        if cols:\n",
    "            return self.data[cols]\n",
    "        return self.data\n",
    "    \n",
    "    def where(self, condition_func):\n",
    "        # Applique une fonction vectorisée pour filtrer\n",
    "        mask = condition_func(self.data)\n",
    "        self.data = self.data[mask]\n",
    "        return self\n",
    "    \n",
    "    def group_by_mean(self, group_col, value_col):\n",
    "        # Implémentation manuelle d'un groupby mean avec numpy\n",
    "        unique_groups = np.unique(self.data[group_col])\n",
    "        results = {}\n",
    "        for group in unique_groups:\n",
    "            mask = self.data[group_col] == group\n",
    "            mean_val = np.mean(self.data[mask][value_col])\n",
    "            results[group] = mean_val\n",
    "        return results\n",
    "\n",
    "# --- Benchmark ---\n",
    "# 1. Notre API Custom\n",
    "print(\"Test API Custom...\")\n",
    "start = time.time()\n",
    "db = TableSQL(df[['Departement', 'Salaire', 'Performance']])\n",
    "# Requête : WHERE Salaire > 60000 GROUP BY Departement AVG(Performance)\n",
    "res_custom = db.where(lambda x: x['Salaire'] > 60000).group_by_mean('Departement', 'Performance')\n",
    "t_custom = time.time() - start\n",
    "\n",
    "# 2. Pandas Query (La référence)\n",
    "print(\"Test Pandas Query...\")\n",
    "start = time.time()\n",
    "res_pandas = df.query(\"Salaire > 60000\").groupby('Departement', observed=True)['Performance'].mean()\n",
    "t_pandas = time.time() - start\n",
    "\n",
    "print(f\"\\nRésultats Benchmark :\")\n",
    "print(f\"Custom Numpy SQL : {t_custom:.5f} s\")\n",
    "print(f\"Pandas Query     : {t_pandas:.5f} s\")\n",
    "print(f\"Vainqueur        : {'Pandas' if t_pandas < t_custom else 'Custom SQL'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
